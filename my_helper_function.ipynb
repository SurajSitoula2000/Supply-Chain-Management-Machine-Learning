{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbdbfcb-9110-466f-8c39-5f170a527d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the future, consider the following..\n",
    "# Class for datetimes already exists\n",
    "# Class for places (locations, countries, continent etc. use googlemaps!)\n",
    "# Plotting Maps etc.\n",
    "# Class for preprocessing data e.g. data cleaning and EDA\n",
    "# All my useful functions in one place!\n",
    "\n",
    "# First, import the coutry code to continent map for my location functions\n",
    "import pandas as pd\n",
    "import os\n",
    "path = os.curdir+\"\\Data\\Source\\\\\"\n",
    "ct_df = pd.read_csv(path+\"country_code_to_continent_map.csv\")\n",
    "CONTINENT_DICT = {x:y for x,y in zip(ct_df.country,ct_df.continent)}\n",
    "\n",
    "###############################################################################\n",
    "# DATA CLEANING & OTHER BASIC FUNCTIONS #\n",
    "# Note: Some functions are for PEPFAR Data ONLY#\n",
    "###############################################################################\n",
    "\n",
    "def getAddress(entity, continent_dict=CONTINENT_DICT):\n",
    "    \"\"\"\n",
    "    Takes in the name of a place and returns a tuple of the formated address, \n",
    "    country (short, and long versions) and continent\n",
    "    \"\"\"\n",
    "    import urllib\n",
    "    import requests\n",
    "    \n",
    "    # Find the URL to communicate with google\n",
    "    main_api = \"http://maps.googleapis.com/maps/api/geocode/json?\"\n",
    "    address = entity #'Charles River Associates, Washington, DC'\n",
    "    \n",
    "    # urlencode puts the %20 for the spaces, makes the data validation nicer\n",
    "    url = main_api + urllib.parse.urlencode({'address': address})\n",
    "   \n",
    "    # Now to go and get jsson response\n",
    "    json_data = requests.get(url).json()\n",
    "    #json_status = json_data['status']\n",
    "      \n",
    "    # Get address\n",
    "    formatted_address = json_data['results'][0]['formatted_address']\n",
    "    # Get continent\n",
    "    items_bool = [x['types'] == ['country', 'political'] for x in json_data['results'][0]['address_components']]\n",
    "    i = items_bool.index(True)\n",
    "    \n",
    "    country_s = json_data['results'][0]['address_components'][i]['short_name']\n",
    "    country_l = json_data['results'][0]['address_components'][i]['long_name']\n",
    "    continent = continent_dict[country_s]\n",
    "    return formatted_address, country_l, continent\n",
    "    #getAddress('Harvard, MA')\n",
    "        \n",
    "def plotFreq(data, column_list, cutoff = -1):\n",
    "    \"\"\"\n",
    "    Takes dataframe, list of target columns and a cutoff \n",
    "    e.g. plot the top 20 frequencies if cut0ff=20\n",
    "    defaults to cutoff = -1 to plot the whole distribution.\n",
    "    Returns charts showing distribution for each of the columns. \n",
    "    Frequency of observations on the y-axis.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    l = len(column_list)/2+1\n",
    "    m = 2\n",
    "    plt.figure(figsize=(12,l*4))\n",
    "    for i in range(len(column_list)):\n",
    "        plt.subplot(l,m,i+1)\n",
    "        unique = len(data[column_list[i]].value_counts())\n",
    "        tit = column_list[i].upper() + \" Freq of \" + str(unique) + \" Unique\"\n",
    "        if cutoff == -1:\n",
    "            data[column_list[i]].value_counts().plot()\n",
    "        else:\n",
    "            data[column_list[i]].value_counts()[:cutoff].plot(kind='barh')\n",
    "        plt.title(tit,x=0.6,y=0.6)\n",
    "        \n",
    "def getTransitMetrics(origin, destination, gmaps_api_key):\n",
    "    \"\"\"\n",
    "    Takes origin and destination and returns distance in kilometers, and time in hours\n",
    "    \"\"\"\n",
    "    import googlemaps\n",
    "                \n",
    "    # Find the URL to communicate with google\n",
    "    #link here: https://developers.google.com/maps/documentation/distance-matrix/\n",
    "    #main_api = 'https://maps.googleapis.com/maps/api/distancematrix/json?'\n",
    "    \n",
    "    gmaps = googlemaps.Client(key=gmaps_api_key)\n",
    "    results = gmaps.distance_matrix(origin, destination)\n",
    "    dist = results['rows'][0]['elements'][0]['distance']['value']/float(1000)\n",
    "    time = results['rows'][0]['elements'][0]['duration']['value']/float(3600)\n",
    "    print(dist, time)\n",
    "    return dist, time\n",
    "\n",
    "#getTransitMetrics('Chikato Primary School', 'Masvingo General Hospital')\n",
    "\n",
    "def pdfMerge(filenames, source_directory, merged_filename):\n",
    "    \"\"\"\n",
    "    Purpose: Uses PyPDF library to merge files.\n",
    "    Inputs: filenames - List of strings. pdf filenames of the source pdfs to be merged in the same directory\n",
    "            source_directory - string, full pathname of the directory housing these files\n",
    "            merged_filename - target filename of the merged pdf document\n",
    "    Returns: A merged pdf document saved in the same directory as source files\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.chdir(source_directory)\n",
    "    \n",
    "    from PyPDF2 import PdfFileMerger, PdfFileReader\n",
    "    merger = PdfFileMerger(strict=False)\n",
    "    for filename in filenames:\n",
    "        merger.append(PdfFileReader(open(filename, 'rb')))\n",
    "    merger.write(merged_filename)\n",
    "\n",
    "def getColumnDataTypes(data):\n",
    "    \"\"\"\n",
    "    Takes dataframe as input, returns columns and datatypes summary. \n",
    "    Category vs. Numerical columns or Discrete vs. Continuous. Also Time Series/Date\n",
    "    \n",
    "    # Notes..df.select_dtypes(include=[], exclude=[])\n",
    "    To select all numeric types use the numpy dtype numpy.number\n",
    "    To select strings you must use the object dtype, but note that this will return all object dtype columns\n",
    "    See the numpy dtype hierarchy\n",
    "    To select datetimes, use np.datetime64, ‘datetime’ or ‘datetime64’\n",
    "    To select timedeltas, use np.timedelta64, ‘timedelta’ or ‘timedelta64’\n",
    "    To select Pandas categorical dtypes, use ‘category’\n",
    "    To select Pandas datetimetz dtypes, use ‘datetimetz’ (new in 0.20.0), or a ‘datetime64[ns, tz]’ string\n",
    "    \"\"\"\n",
    "    # Isolate the different types of colums\n",
    "    # Numeric/Continuous\n",
    "    num_cols = list(data._get_numeric_data().columns)\n",
    "\n",
    "    # Categorical/discrete\n",
    "    cat_d = data.select_dtypes(include=['object', 'category'])    \n",
    "    cat_cols = list(cat_d.columns)\n",
    "    # Date-time\n",
    "    dat_d = data.select_dtypes(include=['datetime', 'datetime64','timedelta', 'timedelta64'])\n",
    "    date_cols = list(dat_d.columns)\n",
    "    \n",
    "    t, n, c, d = len(data.columns), len(num_cols), len(cat_cols), len(date_cols)\n",
    "    print(t,n,c,d)\n",
    "    # Check that we have all 33 columns covered\n",
    "    if n+c+d == t:\n",
    "        print(\"ALL columns accounted for!\")\n",
    "        print (\"Total columns: {} \\n, Numeric columns: {}\\n, Categorical columns: {}\\n, Datetime columns: {}\".format(t, n, c, d))\n",
    "    else:\n",
    "        print(\"Some columns NOT accounted for...\")\n",
    "        print (\"Total columns: {} \\n, Numeric columns: {}\\n, Categorical columns: {}\\n, Datetime columns: {}\".format(t, n, c, d))\n",
    "    return (num_cols, cat_cols, date_cols)\n",
    "\n",
    "def parse_raw_data(ExcelFileObject):\n",
    "    # Some notes:  a problem with the encoding, solution here\n",
    "    #..https://stackoverflow.com/questions/19699367/unicodedecodeerror-utf-8-codec-cant-decode-byte\n",
    "    # raw = pd.read_csv('../Supply Chain/pepfar supply chain.csv', encoding = \"ISO-8859-1\")\n",
    "    # Summary data\n",
    "    full = ExcelFileObject\n",
    "    summary = full.parse(sheetname = full.sheet_names[0])\n",
    "    summary.name='summary'\n",
    "    # Purpose data\n",
    "    purpose = full.parse(sheetname = full.sheet_names[1])\n",
    "    purpose.name = 'purpose'\n",
    "    # Ref is the data dictionary for reference on what the columns mean\n",
    "    ref = full.parse(sheetname = full.sheet_names[2]).iloc[:33,:]\n",
    "    ref.name = 'ref'\n",
    "    # Data if the full data set\n",
    "    data = full.parse(sheetname = full.sheet_names[3])\n",
    "    data.name = 'data'\n",
    "    parsed_data = [summary, purpose, ref, data]\n",
    "    return parsed_data\n",
    "\n",
    "def rename_data_columns(data, newcol_list):\n",
    "    # See the original columns\n",
    "    print(\"Old columns: \",data.columns)\n",
    "    newcol_dict = dict(zip(data.columns, newcol_list))\n",
    "    data.rename(columns=(newcol_dict), inplace =True)\n",
    "    print(\"New columns: \", data.columns)\n",
    "    return data\n",
    "\n",
    "def getReferenceInfo(data, column, ref):\n",
    "    fn = ref[ref['NewColumn'] == column]['FieldName']\n",
    "    dt = ref[ref['NewColumn'] == column]['DataType']\n",
    "    fd = ref[ref['NewColumn'] == column]['FieldDescription']\n",
    "    ft = ref[ref['NewColumn'] == column]['FieldNotes']\n",
    "    print(\"{} \\n=======\\n, {} \\n=======\\n, {} \\n=======\\n, {} \\n=======\\n\\\n",
    "          Examples: \\n{} \\n=======\\n\".format(fn, dt, fd, ft, data[column].head())) \n",
    "    \n",
    "# Separate into blocks by datatype. See the types\n",
    "def get_blocks_by_dtype(data):\n",
    "    \" Gives column type report, returns separated blocks by data type\"\n",
    "    blocks = data.as_blocks()\n",
    "    print(\"Total Number of Columns: {}\\nBreakdown....\\n\".format(len(data.columns)))\n",
    "    for k in blocks.keys():\n",
    "        print(\"Type: {} , Count: {} \\nColumns and null counts---: \\n{}\\n\".format(\n",
    "            k,len(blocks[k].columns),blocks[k].isnull().sum()))\n",
    "    return blocks\n",
    "    \n",
    "def clean_data():\n",
    "    # To implement, this should run another file which does data cleaning\n",
    "    pass\n",
    "\n",
    "def load_clean_data(names_):\n",
    "    \"\"\"\n",
    "    Takes a list of names as parameters , returns a dictionary of dataframes for each of the data pieces available\n",
    "    \"\"\"\n",
    "    chunky_keys = names_\n",
    "    dnames = [\"_\"+str(i)+\"_\"+chunky_keys[i]+\".csv\" for i in range(len(chunky_keys))]  \n",
    "#               ['_0_dnum.csv','_1_dnum_country.csv', '_2_dnum_vendor.csv', '_3_dnum_factory.csv'\n",
    "#               , '_4_dnum_brand.csv', '_5_dnum_molecule_test.csv','_6_dnum_lpifsi.csv',\n",
    "#               '_7_ddate.csv' , '_8_dobject.csv' ]\n",
    "    #encoding = \"ISO-8859-1\"\n",
    "    # Load in all of the feature datasets to date\n",
    "    #dchunks = [None,None,None,None,None,None,None,None,None]\n",
    "    chunky_dict = {x: None for x in chunky_keys}\n",
    "    path = os.curdir+'\\Data\\Features\\\\'\n",
    "    for i in range(len(dnames)):\n",
    "        try:\n",
    "            print(\"trying normal method for: ... \", i)\n",
    "            chunky_dict[chunky_keys[i]] = pd.read_csv(path+dnames[i])\n",
    "        except UnicodeDecodeError:\n",
    "            print(\"Failed with encoding error, trying again for: ... \", i)\n",
    "            chunky_dict[chunky_keys[i]] = pd.read_csv(path+dnames[i], encoding = \"ISO-8859-1\")\n",
    "        print(\"Sucess for: ... \", i)\n",
    "    # Drop the extra column\n",
    "    for d in chunky_dict.values():\n",
    "        d.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    return chunky_dict\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "# FEATURE ENGINEERING #\n",
    "# Feature Creation for Country Stability, Logistics Index, and Factory Location #\n",
    "######################################################################################\n",
    "\n",
    "#import my_helper_functions as mhf\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Load in the main data to be used for merging at the end..\n",
    "# main_data = mhf.load_clean_data()\n",
    "\n",
    "#### 1. Country stability Index   ######\n",
    "\n",
    "def generate_country_stability_features():\n",
    "    \"\"\"\n",
    "    Cleans and transforms data from fragile state index in excel sheets. Returns cleaned\n",
    "    data frame\n",
    "    \"\"\"\n",
    "    # From fragile state data (FundForPeace)\n",
    "    # Read in the files \n",
    "    import os\n",
    "    path = os.curdir+'\\Data\\Source\\\\'\n",
    "    fsi_n = ['fsi-2006.xlsx','fsi-2007.xlsx','fsi-2008.xlsx','fsi-2009.xlsx','fsi-2010.xlsx','fsi-2011.xlsx'\n",
    "    ,'fsi-2012.xlsx','fsi-2013.xlsx','fsi-2014.xlsx','fsi-2015.xlsx','fsi-2016.xlsx','fsi-2017.xlsx']\n",
    "    fsi_xl = [pd.read_excel(pd.ExcelFile(path+f)) for f in fsi_n]\n",
    "\n",
    "    # trim the dataframe\n",
    "    fsi_dfs = [d[['Country','Year','Total']] for d in fsi_xl]\n",
    "    for df in fsi_dfs:\n",
    "        df['year'] = df['Year'].apply(lambda x: str(x.year))\n",
    "        df['country'] = df['Country'].str.strip()\n",
    "        df.rename(columns={'Total':'fsi'}, inplace=True)\n",
    "        df.drop(['Year', 'Country'], axis=1, inplace=True)\n",
    "\n",
    "    # Now concatenate vertically!\n",
    "    [len(d) for d in fsi_dfs]\n",
    "\n",
    "    # concatenate to final \n",
    "    fsi_all = pd.concat(fsi_dfs, axis=0)\n",
    "\n",
    "    # Save to disk , ready for use. Consider joining all country statistics at some point?\n",
    "    # Both origin and destination\n",
    "    #fsi_all.to_csv('fsi_2006-2017.csv')\n",
    "    return fsi_all\n",
    "\n",
    "###### 2. Logistics Index   ######\n",
    "\n",
    "# series to be combined into data frames later\n",
    "def compare_columns(data1, data2,column1,column2):\n",
    "    \"\"\"\n",
    "    Takes two dataframes and a column from each. Returns dataframe of the two columns, and prints out the \n",
    "    comparisons.\n",
    "    \"\"\"\n",
    "    fsi_all,lpi_all = data1, data2\n",
    "    fsi_c, lpi_c = pd.Series(fsi_all[column1].unique(), name='fsi'),pd.Series(lpi_all[column2].unique(), name='lpi')\n",
    "    print(\"data1 shape:\",len(fsi_c),\"data2 shape:\",len(lpi_c))\n",
    "    # merge countries\n",
    "    df_country = pd.merge(pd.DataFrame(fsi_c),pd.DataFrame(lpi_c), left_on='fsi', right_on='lpi'\n",
    "             , suffixes=('_fsi', '_lsi'), how='outer')\n",
    "    print(df_country.shape)\n",
    "    #list\n",
    "    lpi_not_fsi = list(df_country[df_country.fsi.isnull()].lpi)\n",
    "    fsi_not_lpi = list(df_country[df_country.lpi.isnull()].fsi)\n",
    "    print(len(lpi_not_fsi), len(fsi_not_lpi))\n",
    "    print(\"In data2 but not in data1: {} \\n----\\nIn data1 but not in data2: {}\".format(\n",
    "        lpi_not_fsi, fsi_not_lpi))\n",
    "    return df_country\n",
    "\n",
    "def generate_country_logistics_features():\n",
    "    \"\"\"\n",
    "    Cleans and transforms excel file with several logistics indicators for countries\n",
    "    Returns cleaned dataframe \n",
    "    \"\"\"\n",
    "    # load the data\n",
    "    import os\n",
    "    path = os.curdir+'\\Data\\Source\\\\'\n",
    "    lpi = pd.ExcelFile(path+'International_LPI_from_2007_to_2016.xlsx')\n",
    "\n",
    "    # iterate over dfs, pull and rename\n",
    "    lpi_dfs = []\n",
    "    for s in lpi.sheet_names:\n",
    "        df_ =lpi.parse(sheetname=s).reset_index()\n",
    "        df_.columns = [s+\"&\"+c for c in df_.columns]\n",
    "        df_.rename(columns={s+'&'+'country':'country',s+'&'+'index':'index'}, inplace=True)\n",
    "        lpi_dfs.append(df_.drop('index', axis=1))\n",
    "    [d.shape for d in lpi_dfs]\n",
    "\n",
    "    lpicountries=[]\n",
    "    for d in lpi_dfs:\n",
    "        lpicountries += list(lpi_dfs[0].country)\n",
    "    lpicountries = set(lpicountries)\n",
    "    print(len(lpicountries))\n",
    "    # Rename and prepare to merge all dfs horizontally\n",
    "    ct = pd.DataFrame([], lpicountries).reset_index().rename(columns={'index':'country'})\n",
    "    # Iterate and merge\n",
    "    for d in lpi_dfs:\n",
    "        ct = ct.merge(d,how='left',left_on='country', right_on='country' )\n",
    "    print(\"shape ct: \",ct.shape)\n",
    "\n",
    "    lpi_cols = list(set([x.split('&')[1] for x in ct.columns[1:]]))\n",
    "    mis = ['2006', '2008', '2009', '2011', '2013', '2015']\n",
    "    missing_cols = [x+'&'+y for x in mis for y in lpi_cols]\n",
    "    print(\"missing_cols:\", len(missing_cols))\n",
    "    ''' Mapping...\n",
    "    2006 --> 2007\n",
    "    2008 --> (2010-2007)/3+2007\n",
    "    2009 --> 2*(2010-2007)/3+2007\n",
    "    2011 --> (2010+2012)/2\n",
    "    2013 --> (2012+2014)/2\n",
    "    2015 --> (2014+2016)/2\n",
    "    '''\n",
    "    y='2006'\n",
    "    sub = [x for x in missing_cols if x[:4]==y]\n",
    "    for c in sub:\n",
    "        attr = c.split('&')[1]\n",
    "        ct[c] = ct['2007&'+attr]\n",
    "    y='2008'\n",
    "    sub = [x for x in missing_cols if x[:4]==y]\n",
    "    for c in sub:\n",
    "        attr = c.split('&')[1]\n",
    "        ct[c] = (ct['2010&'+attr]-ct['2007&'+attr])/3.0 + ct['2007&'+attr]\n",
    "    y='2009'\n",
    "    sub = [x for x in missing_cols if x[:4]==y]\n",
    "    for c in sub:\n",
    "        attr = c.split('&')[1]\n",
    "        ct[c] = 2*(ct['2010&'+attr]-ct['2007&'+attr])/3.0 + ct['2007&'+attr]\n",
    "    y='2011'\n",
    "    sub = [x for x in missing_cols if x[:4]==y]\n",
    "    for c in sub:\n",
    "        attr = c.split('&')[1]\n",
    "        ct[c] = (ct['2010&'+attr]+ct['2012&'+attr])/2.0\n",
    "    y='2013'\n",
    "    sub = [x for x in missing_cols if x[:4]==y]\n",
    "    for c in sub:\n",
    "        attr = c.split('&')[1]\n",
    "        ct[c] = (ct['2012&'+attr]+ct['2014&'+attr])/2.0    \n",
    "    y='2015'\n",
    "    sub = [x for x in missing_cols if x[:4]==y]\n",
    "    for c in sub:\n",
    "        attr = c.split('&')[1]\n",
    "        ct[c] = (ct['2014&'+attr]+ct['2016&'+attr])/2.0\n",
    "    ct.shape\n",
    "\n",
    "    for col in ct.columns[1:]:\n",
    "        m = ct[col].mean()\n",
    "        ct[col].fillna(m, inplace=True)\n",
    "\n",
    "    stacked = ct.set_index(['country']).stack().reset_index()\n",
    "    stacked['year'] = [x[0] for x in stacked.level_1.str.split('&')]\n",
    "    stacked['score_type'] = [x[1] for x in stacked.level_1.str.split('&')]\n",
    "    stacked.drop('level_1', axis=1, inplace=True)\n",
    "    stacked.rename(columns={0:'score'}, inplace=True)\n",
    "    unstacked = stacked.set_index(['country','year', 'score_type']).unstack().reset_index()\n",
    "    unstacked.columns= ['country', 'year', 'customs' ,'infra', 'intl_ship'\n",
    "                        , 'logistic_qlty', 'lpi', 'timeliness','track_trace']\n",
    "    print(unstacked.shape)\n",
    "    unstacked.head()\n",
    "\n",
    "    lpi_all =unstacked\n",
    "    #lpi_all.to_csv('lpi_2006-2016.csv')\n",
    "    print(\"lpi describe: \", lpi_all.describe())\n",
    "    return lpi_all\n",
    "\n",
    "###### 3. Harmonize and join lpi and fsi with main data!  ######\n",
    "\n",
    "def combine_logistics_and_stability_features(lpi_all, fsi_all):\n",
    "    \"\"\" \n",
    "    Takes the two dataframes of the stability and logistics indices and combines\n",
    "    Returns dataframe of combined\n",
    "    \"\"\"\n",
    "    ### Harmonize the country_names\n",
    "    print(\"\\nCountry names to harmonize: \", lpi_all.country.unique(), fsi_all.country.unique())\n",
    "    print(\"\\nFSI Year Value counts: \",fsi_all.year.value_counts().sort_index())\n",
    "    print(\"\\nFSI Year Value counts: \",lpi_all.year.value_counts().sort_index())\n",
    "\n",
    "    import os\n",
    "    path = os.curdir+'\\Data\\Features\\\\'\n",
    "    df_country = compare_columns(fsi_all, lpi_all,'country','country')\n",
    "    df_country.to_csv(path+'fsi_lpi_country comps.csv')\n",
    "    # After doing data cleaning in Excel, get the resulting map\n",
    "    map1 = pd.ExcelFile(path+'fsi_lpi_country comps_map.xlsx').parse(sheetname='fsi-lpi-map')\n",
    "    # Change lpi to match fsi\n",
    "    lpi_all['country'].replace({x:y for x,y in zip(map1.lpi, map1.fsi)}, inplace=True)\n",
    "    #df_random = compare_columns(fsi_all, lpi_all,'country','country')\n",
    "    \n",
    "    # Now combine lpi and fsi\n",
    "    print(lpi_all.shape, fsi_all.shape)\n",
    "    lpi_fsi_combined = pd.merge(fsi_all, lpi_all, how='left', left_on=['country', 'year'],\n",
    "                               right_on=['country', 'year'])\n",
    "    lpi_fsi_combined['year'] = pd.to_datetime(lpi_fsi_combined['year'])\n",
    "    lpi_fsi_combined['year'] = [x.year for x in lpi_fsi_combined['year']]\n",
    "    print(lpi_fsi_combined.shape)\n",
    "    lpi_fsi_combined.describe()\n",
    "    lpi_fsi_combined.to_csv(path+'lpi_fsi_combined.csv')\n",
    "    return lpi_fsi_combined\n",
    "\n",
    "\n",
    "###### 4. Origin Factory Address, Country, and Continent ##########\n",
    "\n",
    "def generate_factory_location_features(main_data):\n",
    "    \"\"\"\n",
    "    Takes in main data on supply chain, looksup addresses and locations using the googlemaps API\n",
    "    Returns dataframe with factory, country and continent included \n",
    "    \"\"\"\n",
    "    ### Using factory to extract address and then, country maybe even distance and time to travel  \n",
    "    factory = pd.DataFrame(main_data['factory'].unique(),columns=['factory'] )\n",
    "    factory.head()\n",
    "    # Generate list of corresponding addresses\n",
    "    factory_address = []\n",
    "    for f in factory['factory']:\n",
    "        try:\n",
    "            factory_address.append(getAddress(f))\n",
    "        except IndexError:\n",
    "            x = f.split()\n",
    "            try:\n",
    "                factory_address.append(getAddress(\" \".join(x[-3:])))\n",
    "            except IndexError:\n",
    "                try:\n",
    "                    factory_address.append(getAddress(\" \".join(x[-2:])))\n",
    "                except IndexError:\n",
    "                    try:\n",
    "                        factory_address.append(getAddress(\" \".join(x[-1:])))\n",
    "                    except IndexError:\n",
    "                        factory_address.append((\"IndexError\",\"IndexError\",\"IndexError\"))\n",
    "    # Check the length\n",
    "    print('factory_address:', len(factory_address), \"; factory:\", len(factory))\n",
    "\n",
    "    # Make sure addresses and factories have the same length\n",
    "    assert len(factory_address) == len(factory), \"Length Mismatch!\"\n",
    "    factory_address_df = pd.DataFrame(factory_address)\n",
    "    # Concatenate the address to the factory\n",
    "    factory['factory_address'], factory['origin_country'], factory['origin_continent'] = factory_address_df[0], \\\n",
    "    factory_address_df[1], factory_address_df[2]\n",
    "    # How many factories were not identified/located?\n",
    "    print(factory[factory['factory_address'] == 'IndexError']['factory_address'].value_counts(),'\\n')\n",
    "    # The unidentified factories..\n",
    "    print('The unidentified factories...\\n',list(factory[factory['factory_address'] == 'IndexError']['factory']))\n",
    "    factory.origin_continent.value_counts()\n",
    "\n",
    "    # Fix the missing ones..\n",
    "    import os\n",
    "    path = os.curdir + '\\Data\\Features\\\\'\n",
    "    #fact_ref = pd.read_csv(path+'factory_map_premade.csv', encoding = \"ISO-8859-1\")\n",
    "    fact_ref = pd.read_excel(path+'factory_map_premadeX.xlsx', encoding = \"ISO-8859-1\")\n",
    "    factory_misfits_dict = {x:y for x, y in zip(fact_ref.name,fact_ref.factory_address) \n",
    "    if x in list(factory[factory['factory_address'] == 'IndexError']['factory'])}\n",
    "    # Make the factory name an index of the factory df\n",
    "    factory.index = factory['factory']\n",
    "    try:\n",
    "        factory['factory_address'] = factory['factory'].apply(\n",
    "                lambda x: factory_misfits_dict.get(x, factory.loc[x]['factory_address']))\n",
    "    except NameError:\n",
    "        pass\n",
    "    \n",
    "    # Massage the data..\n",
    "    # rename with better names\n",
    "    factory['name'] = factory.index\n",
    "    # drop redundant factory column\n",
    "    factory.drop(['factory'], axis = 1, inplace=True)\n",
    "    # intro numerical index to replace the other\n",
    "    factory.index = range(len(factory))\n",
    "    # Which indices have the error on continent and country?\n",
    "    no_country_idx  = list(factory[factory.origin_continent=='IndexError']['factory_address'].index)\n",
    "    # Make a country to continent map\n",
    "    continent_dict = dict(zip(factory.origin_country, factory.origin_continent))\n",
    "\n",
    "    # Update the missing specifc places for country and continent\n",
    "    for i in no_country_idx:\n",
    "        ctry = factory.iloc[i]['factory_address'].split()[-1]\n",
    "        cntnt = getAddress(ctry)[2]\n",
    "        factory.iloc[i]['origin_country'] = ctry\n",
    "        factory.iloc[i]['origin_continent'] = cntnt\n",
    "    import os\n",
    "    path = os.curdir+'\\Data\\Features\\\\'\n",
    "    factory.to_csv(path+\"factory_country_continent.csv\")\n",
    "    print(\"factory shape: \",factory.shape)\n",
    "    factory.head()\n",
    "\n",
    "    # Check countries across two list of supply and demand\n",
    "    orig_list = list(factory['origin_country'])\n",
    "    dest_list = list(main_data.country)\n",
    "    # Countries which are both origins and destinations\n",
    "    print(\"Countries in both origin and destination: \",sorted([c for c in orig_list if c in dest_list]))\n",
    "    # Drop dups\n",
    "    factory = factory.drop_duplicates(keep='first')\n",
    "    return factory\n",
    "\n",
    "def add_factory_origin_features(main_data, factory): \n",
    "    return pd.merge(main_data, factory, how=\"left\", left_on=['factory'], right_on=['name'])\n",
    "\n",
    "def destination_and_origin_lpi_fsi_indicators(main_data, lpi_fsi_combined): # Must have origin factory/country first\n",
    "    \"\"\"\n",
    "    Takes in main data on supply chain as well as combined lpi_fsi_data, \n",
    "    harmonizes the country names and returns 2 dfs, one for origin and another for \n",
    "    destination metrics for both stability and logistics indices\n",
    "    \"\"\"\n",
    "    # First hamonize with main data\n",
    "    #------------------------------------------------#\n",
    "    dest = main_data.groupby(['country', 'del_date_scheduled_yr']).agg(['count', 'sum', 'mean'])['delayed']\n",
    "    orig = main_data.groupby(['origin_country', 'del_date_scheduled_yr']).agg(['count', 'sum', 'mean'])['delayed']\n",
    "    dest.reset_index(inplace=True)\n",
    "    orig.reset_index(inplace=True)\n",
    "    #orig['origin_country'] = orig.country\n",
    "    #orig.drop('country', axis=1)\n",
    "    print(\"Destination countries data: \",dest.shape\n",
    "          ,\"Unique countries: \",dest.country.unique()\n",
    "          ,\"Origin countries data: \", orig.shape\n",
    "         ,\"Unique countries: \", orig.origin_country.unique())\n",
    "\n",
    "    compare_columns(dest,lpi_fsi_combined, 'country', 'country')\n",
    "\n",
    "    lpi_fsi_combined['country'].replace({'Congo Democratic Republic':'Congo, DRC'\n",
    "    ,\"Cote d'Ivoire\":\"Côte d'Ivoire\", 'Kyrgyz Republic':'Kyrgyzstan'}, inplace=True)\n",
    "\n",
    "    compare_columns(dest, lpi_fsi_combined, 'country', 'country')\n",
    "    compare_columns(orig, lpi_fsi_combined, 'origin_country', 'country')\n",
    "    #------------------------------------------------#\n",
    "    \n",
    "    destination_metrics_by_year = pd.merge(dest,lpi_fsi_combined, how='left'\n",
    "               , left_on=['country','del_date_scheduled_yr'], right_on=['country','year'])\n",
    "    print(destination_metrics_by_year.columns,destination_metrics_by_year.describe(),\n",
    "         destination_metrics_by_year.head())\n",
    "    #destination_metrics_by_year.to_csv('destination_metrics_by_year.csv')\n",
    "\n",
    "    origin_metrics_by_year = pd.merge(orig,lpi_fsi_combined, how='left'\n",
    "               , left_on=['origin_country','del_date_scheduled_yr'], right_on=['country','year'])\n",
    "    destination_metrics_by_year = destination_metrics_by_year.drop_duplicates(keep='first')\n",
    "    origin_metrics_by_year = origin_metrics_by_year.drop_duplicates(keep='first')\n",
    "    #print(origin_metrics_by_year.columns,origin_metrics_by_year.describe(),\n",
    "    #     origin_metrics_by_year.head())\n",
    "    #origin_metrics_by_year.to_csv('origin_metrics_by_year.csv')\n",
    "    return destination_metrics_by_year, origin_metrics_by_year\n",
    "\n",
    "def add_lpi_fsi_features(main_data, origin_metrics, destination_metrics): # Must Happen after they are generated!\n",
    "    \"\"\"\n",
    "    Adds newly generated destination and origin country logistics and country fragility/peace features \n",
    "    to the main data.\n",
    "    \"\"\"\n",
    "    # Prep the datarames\n",
    "    orig, dest = origin_metrics, destination_metrics\n",
    "    dest = dest.groupby(['dest_country', 'dest_del_date_scheduled_yr']).agg('mean')\n",
    "    orig = orig.groupby(['orig_origin_country', 'orig_del_date_scheduled_yr']).agg('mean')\n",
    "    dest, orig = dest.reset_index(), orig.reset_index()\n",
    "    # Merge the data consecutively\n",
    "    # Join the odd ones into dobject\n",
    "    do1 = pd.merge(main_data,dest, how='left',left_on=['country', 'del_date_scheduled_yr']\n",
    "             , right_on=['dest_country', 'dest_del_date_scheduled_yr'])\n",
    "    do2 = pd.merge(do1, orig, how='left',left_on=['origin_country', 'del_date_scheduled_yr']\n",
    "             , right_on=['orig_origin_country', 'orig_del_date_scheduled_yr'])\n",
    "    main_data = do2.copy()    \n",
    "    return main_data\n",
    "\n",
    "def country_metrics_corr(data):\n",
    "    \"\"\" \n",
    "    Takes in data with destination or origin metrics aggregated\n",
    "    Retruns heatmap of correlations for visualizing. Compares against mean, sum and count\n",
    "    \"\"\"\n",
    "    drop = [x for x in data.columns if (\"sum\" in x) or (\"count\" in x)]\n",
    "    d=data.drop(drop, axis=1)\n",
    "    f, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "    #print(d.corr())\n",
    "    sns.heatmap(d.corr())\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "        # -------- ML MODEL DEVELOPMENT AND REFINEMENT --------------#\n",
    "###############################################################################\n",
    "from sklearn.pipeline import TransformerMixin\n",
    "\n",
    "class Dummifier(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Creates dummies from DataFrame, returns dataframe with first level of dummies dropped\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return pd.get_dummies(X, drop_first=True)\n",
    "    \n",
    "class Labeler(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Creates labels for series, returns dataframe with 0,1\n",
    "    \"\"\"\n",
    "    def fit(self, y, X=None):\n",
    "        return self\n",
    "    def transform(self, y):\n",
    "        from sklearn.preprocessing import LabelBinarizer\n",
    "        enc = LabelBinarizer()\n",
    "        return enc.fit_transform(y)\n",
    "    \n",
    "# Extract feature importances into a df\n",
    "def plot_feature_importances(fitted_estimator, X_train, n_features, show_plot=True):\n",
    "    \"\"\"\n",
    "    Takes in an estimator RFClassifier which has already been \"fitted\" with feature importances, the full data\n",
    "    Plots the top n_features importance. Returns full dataframe of importances by feature, and importance from n_features\n",
    "    \"\"\"\n",
    "    features_clf = X_train.columns\n",
    "    key_features_clf = pd.DataFrame(fitted_estimator.feature_importances_, features_clf)\n",
    "    key_features_clf.reset_index(inplace=True)\n",
    "    key_features_clf.columns=['feature', 'importance']\n",
    "    dfresult = key_features_clf.set_index('feature').sort_values('importance', ascending=False)[:n_features]\n",
    "    print(\"Total Importance of {} features: {}\".format(n_features,dfresult.sum()))\n",
    "    if show_plot:\n",
    "        dfresult.plot(kind=\"barh\", ylim=(0,0.5))\n",
    "    return key_features_clf.set_index('feature').sort_values('importance', ascending=False)\n",
    "\n",
    "def pca_results(good_data, pca, min_feature_influence=0.1):\n",
    "    '''\n",
    "    Create a DataFrame of the PCA results\n",
    "    Includes dimension feature weights and explained variance\n",
    "    Visualizes the PCA results\n",
    "    '''\n",
    "    import numpy as np\n",
    "    # Dimension indexing\n",
    "    dimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n",
    "    # PCA components\n",
    "    components = pd.DataFrame(np.round(pca.components_, 4), columns = good_data.keys())\n",
    "    components.index = dimensions\n",
    "    # PCA explained variance\n",
    "    ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n",
    "    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n",
    "    variance_ratios.index = dimensions\n",
    "    # Create a bar plot visualization\n",
    "    fig, ((ax0,ax1),(ax2,ax3)) = plt.subplots(\n",
    "        nrows=2,ncols=2,figsize = (14,8))\n",
    "    axes = [ax0,ax1,ax2,ax3]\n",
    "    # Plot the feature weights as a function of the components  506/333\n",
    "    for d in dimensions: # for each dimension\n",
    "        i = dimensions.index(d)\n",
    "        idx =abs(components.loc[d,:])>min_feature_influence # Select most influential features\n",
    "        components.loc[d,:][idx].sort_values(ascending=False).plot(\n",
    "            kind=\"barh\",ax=axes[i], title=\"Important Featues for: \"+d)  #plot on separate axis\n",
    "        axes[i].set_xlabel(\"Feature Weights\")\n",
    "    # Return a concatenated DataFrame\n",
    "    pd.concat([variance_ratios, components], axis = 1)\n",
    "\n",
    "def train_test_oversample(X, y, test_size=0.35, use_smote=False):\n",
    "    \"\"\"\n",
    "    Returns oversampled X and y dataframes depending on key word args.\n",
    "    \"\"\"\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    # Train-Test split\n",
    "    X_tr_clf_res, X_ts_clf_res, y_tr_clf_res, y_ts_clf_res = train_test_split(\n",
    "            X,y,test_size=test_size, random_state=121)\n",
    "\n",
    "    smote = SMOTE(random_state=121, ratio = 1.0)\n",
    "    print(\"XTrain, yTrain shapes BEFORE Oversampling: {} and {}\".format(X_tr_clf_res.shape, y_tr_clf_res.shape))\n",
    "    if use_smote: # do oversample technique\n",
    "        X_tr_clf_res, y_tr_clf_res = smote.fit_sample(X_tr_clf_res, y_tr_clf_res)\n",
    "    print(\"XTrain, yTrain shapes AFTER Oversampling: {} and {}\".format(X_tr_clf_res.shape, y_tr_clf_res.shape))\n",
    "    # Convert to dataframes for helper formulae\n",
    "    X_tr_clf_res = pd.DataFrame(X_tr_clf_res, columns=X.columns)\n",
    "    y_tr_clf_res = pd.DataFrame(y_tr_clf_res, columns=y.columns)\n",
    "    print(\"Shape of XTrain: {} yTrain: {} XTest: {} yTest: {}\".format(\n",
    "        X_tr_clf_res.shape, X_ts_clf_res.shape, y_tr_clf_res.shape, y_ts_clf_res.shape))\n",
    "    return X_tr_clf_res, X_ts_clf_res, y_tr_clf_res, y_ts_clf_res\n",
    "\n",
    "\n",
    "def train_test_conditional(X, y, df_true_pos, ddate, delayed):\n",
    "    \"\"\"\n",
    "    Takes several datarames, output variable and return conditinal train test split sets.\n",
    "    \"\"\"\n",
    "    # Set the data arrays right..train test split\n",
    "    print(\"Now getting train test splits for regression...\")\n",
    "    X_tr_reg_all = X.loc[delayed[delayed==1].dropna().index.tolist(),:]\n",
    "    X_tr_reg = X_tr_reg_all.drop(df_true_pos.index.tolist(),axis=0)\n",
    "\n",
    "    y_tr_reg_all = ddate.loc[delayed[delayed==1].dropna().index.tolist(),['delivery_delay_time']]\n",
    "    y_tr_reg_tp = y_tr_reg_all.drop(df_true_pos.index.tolist(),axis=0)\n",
    "    y_tr_reg = y_tr_reg_tp.delivery_delay_time.dt.days\n",
    "\n",
    "    # Do the train test split business\n",
    "    X_ts_reg = X.loc[df_true_pos.index.tolist(),:]\n",
    "    y_ts_reg = ddate.loc[df_true_pos.index.tolist(),['delivery_delay_time']]['delivery_delay_time'].dt.days\n",
    "    print(\"Shapes:\\n {}\\n,{}\\n,{}\\n,{}\".format(\n",
    "                    X_tr_reg.shape, X_ts_reg.shape, y_tr_reg.shape, y_ts_reg.shape))\n",
    "    return X_tr_reg, X_ts_reg, y_tr_reg, y_ts_reg\n",
    "\n",
    "def fit_and_generate_true_positives(estimator, X_tr_clf_res, X_ts_clf_res, y_tr_clf_res, y_ts_clf_res):\n",
    "    \"\"\"\n",
    "    ?Returns dataframe of true positives and dataframe of actual vs. pred\n",
    "    \"\"\"\n",
    "    # Fit the regression to the data, training!\n",
    "    clf = estimator\n",
    "    clf.fit(X_tr_clf_res, y_tr_clf_res)\n",
    "    # Predicts test values, Accuracy and error calcs\n",
    "    y_pred_clf = clf.predict(X_ts_clf_res)\n",
    "    # Make dataframe of delayed vs. predicted\n",
    "    df_pred=pd.DataFrame(y_ts_clf_res, columns=['delayed']); df_pred['pred']= y_pred_clf\n",
    "    # Put all the corectly identified delayed items into one dataframe \n",
    "    # To be used in the prediction of length/extent of delay \n",
    "    df_true_pos = df_pred[(df_pred.delayed==1) & (df_pred.delayed==df_pred.pred)]\n",
    "    print(\"Shape of true positive df: \", \n",
    "              df_true_pos.shape,\"Number of 1's in true positive df:\", df_true_pos.sum())\n",
    "    return df_true_pos, df_pred\n",
    "\n",
    "# The model selection function from my helper functions\n",
    "def model_selection(X_train, X_test, y_train, y_test, estimator, alg_type):\n",
    "    \"\"\"\n",
    "    Takes train and test data sets for both features and target plus an estimator and \n",
    "    returns f1_score or a tuple of r2 and RMSE. So be careful which alg_type you want.\n",
    "    \"\"\"\n",
    "    # Scoring functions and some estimator built in container\n",
    "    from sklearn.metrics import f1_score, r2_score, mean_squared_error\n",
    "    from sklearn.pipeline import Pipeline#, preprocessing\n",
    "    from sklearn import preprocessing\n",
    "    import numpy as np\n",
    "\n",
    "    model = Pipeline([ #('label_encoding', EncodeCategorical(X.keys())),\n",
    "         #('one_hot_encoder', OneHotEncoder()),\n",
    "         ('estimator', estimator)])    \n",
    "    if alg_type == 'clf':\n",
    "        # Instantiate the classification model and visualizer\n",
    "        y_train = preprocessing.LabelEncoder().fit_transform(y_train.values.ravel())\n",
    "        y_test = preprocessing.LabelEncoder().fit_transform(y_test.values.ravel())\n",
    "        model.fit(X_train, y_train)\n",
    "        expected  = y_test\n",
    "        predicted = model.predict(X_test)\n",
    "        # Compute and return the F1 score (the harmonic mean of precision and recall)\n",
    "        return (f1_score(expected, predicted))\n",
    "    elif alg_type == 'reg':\n",
    "        y_train = y_train.values.ravel()\n",
    "        y_test = y_test.values.ravel()\n",
    "        model.fit(X_train, y_train)\n",
    "        expected  = y_test\n",
    "        predicted = model.predict(X_test)\n",
    "        # Compute and return the R2 and RMSE metrics\n",
    "        r = (r2_score(expected, predicted), np.sqrt(mean_squared_error(expected, predicted)))\n",
    "        return r\n",
    "\n",
    "def visual_model_selection(X_train, X_test, y_train, y_test, estimator, show_plot=True):\n",
    "    \"\"\"\n",
    "    Takes train and test data sets for both features and target plus an estimator and \n",
    "    returns a visual classification report.\n",
    "    \"\"\" \n",
    "    from sklearn.pipeline import Pipeline \n",
    "    from yellowbrick.classifier import ClassificationReport\n",
    "    #y_train = preprocessing.LabelEncoder().fit_transform(y_train.values.ravel())\n",
    "    #y_test = preprocessing.LabelEncoder().fit_transform(y_test.values.ravel())\n",
    "        \n",
    "    model = Pipeline([('estimator', estimator)])\n",
    "\n",
    "    # Instantiate the classification model and visualizer\n",
    "    visualizer = ClassificationReport(model, classes=['on-time', 'delayed'])\n",
    "    visualizer.fit(X_train, y_train)\n",
    "    visualizer.score(X_test, y_test)\n",
    "    visualizer.poof()\n",
    "    return visualizer.scores\n",
    "\n",
    "def run_combined_classify_regress_model(data, ddate, delayed,classifier, regressor\n",
    "                                        , test_size=0.35, use_smote=False, show_plot=False):\n",
    "    \"\"\"\n",
    "    Combined model which classifies and then does regression to find length of delay. \n",
    "    Plots several useful metrics for classification and regression.\n",
    "    Saves the predicted vs. actual and true positive preditions to Data\\Results folder\n",
    "    Returns key dataframes for analysis\n",
    "        df_pred_fin ----------> final predictions on the test observations \n",
    "        df_true_pos_fin ------> true postive predictions\n",
    "        d_feat_imp_clf_fin ---> feature importances for the classification model\n",
    "        d_rsq_fin ------------> regression scores R2 and RMSE\n",
    "        clf_object -------> is the classification prediction object\n",
    "    \"\"\" \n",
    "    #-------------------Classification----------------#\n",
    "    # Get the train test splits # Use Oversampling\n",
    "    X_tr_clf_fin,X_ts_clf_fin,y_tr_clf_fin,y_ts_clf_fin = train_test_oversample(data, delayed\n",
    "                                                            , test_size=test_size, use_smote=use_smote)\n",
    "    # Instatiatiate the models\n",
    "    final_estimator = classifier\n",
    "    print(\"\\n----\\n\")\n",
    "    # Fit the regression to the data and train\n",
    "    final_clf = final_estimator().fit(X_tr_clf_fin, y_tr_clf_fin)\n",
    "    try:\n",
    "        d_feat_imp_clf_fin = plot_feature_importances(final_clf, X_tr_clf_fin, 30,show_plot=show_plot)\n",
    "    except AttributeError:\n",
    "        print(\"Classifier has no feature importance attributes\")\n",
    "        d_feat_imp_clf_fin = pd.DataFrame([[]])\n",
    "    # Predict and plot F1-Scores, Precision and Recall\n",
    "    plt.subplots(figsize=(6,5))\n",
    "    clf_scores = visual_model_selection(X_tr_clf_fin, X_ts_clf_fin, y_tr_clf_fin\n",
    "                                        , y_ts_clf_fin, final_clf, show_plot=show_plot)\n",
    "    df_true_pos_fin, df_pred_fin = fit_and_generate_true_positives(final_estimator, X_tr_clf_fin\n",
    "                                                                 , X_ts_clf_fin, y_tr_clf_fin, y_ts_clf_fin)\n",
    "    # Save to disk\n",
    "    path = os.curdir+\"\\Data\\Results\\\\\"\n",
    "    df_pred_fin.to_csv(path+'classifier_final_predicted.csv')\n",
    "    df_true_pos_fin.to_csv(path+'classifier_final_true_positives.csv') # Save to data folders\n",
    "    \n",
    "    #-------------------Regression----------------#\n",
    "    reg_estimator_fin = regressor\n",
    "    # Get training and test sets...\n",
    "    X_tr_reg_fin, X_ts_reg_fin, y_tr_reg_fin, y_ts_reg_fin = train_test_conditional(data,\n",
    "                                                                delayed, df_true_pos_fin, ddate, delayed)\n",
    "    rsq = []\n",
    "    rsq.append(model_selection(X_tr_reg_fin, X_ts_reg_fin\n",
    "                                   , y_tr_reg_fin, y_ts_reg_fin, reg_estimator_fin(), 'reg'))\n",
    "    d_rsq_fin = pd.DataFrame(rsq, columns=['r2', 'rmse'])\n",
    "    return df_pred_fin, df_true_pos_fin,  d_feat_imp_clf_fin, clf_scores, d_rsq_fin \n",
    "\n",
    "def run_combined_classify_regress_model_prefit(data, ddate, delayed,classifier, regressor\n",
    "                                        , test_size=0.35, use_smote=False, show_plot=False):\n",
    "    \"\"\"\n",
    "    Combined model which classifies and then does regression to find length of delay. \n",
    "    Plots several useful metrics for classification and regression.\n",
    "    Saves the predicted vs. actual and true positive preditions to Data\\Results folder\n",
    "    Returns key dataframes for analysis\n",
    "        df_pred_fin ----------> final predictions on the test observations \n",
    "        df_true_pos_fin ------> true postive predictions\n",
    "        d_feat_imp_clf_fin ---> feature importances for the classification model\n",
    "        d_rsq_fin ------------> regression scores R2 and RMSE\n",
    "        clf_object -------> is the classification prediction object\n",
    "    \"\"\" \n",
    "    from sklearn.metrics import r2_score, mean_squared_error, classification_report, confusion_matrix\n",
    "    import numpy as np\n",
    "    #-------------------Classification----------------#\n",
    "    # Get the train test splits # Use Oversampling\n",
    "    X_tr_clf_fin,X_ts_clf_fin,y_tr_clf_fin,y_ts_clf_fin = train_test_oversample(data, delayed\n",
    "                                                            , test_size=test_size, use_smote=use_smote)\n",
    "    # Instatiatiate the models\n",
    "    final_estimator = classifier\n",
    "    print(\"\\n----\\n\")\n",
    "    # Fit the regression to the data and train\n",
    "    final_clf = final_estimator.fit(X_tr_clf_fin, y_tr_clf_fin)\n",
    "    try:\n",
    "        d_feat_imp_clf_fin = plot_feature_importances(final_clf, X_tr_clf_fin, 30,show_plot=show_plot)\n",
    "    except AttributeError:\n",
    "        print(\"Classifier has no feature importance attributes\")\n",
    "        d_feat_imp_clf_fin = pd.DataFrame([[]])\n",
    "    # Predict and plot F1-Scores, Precision and Recall    \n",
    "    clfreport, cmatrix=[m(y_ts_clf_fin,final_estimator.predict(X_ts_clf_fin)) \n",
    "                        for m in [classification_report, confusion_matrix] ]\n",
    "    # Make dataframe of delayed vs. predicted\n",
    "    df_pred_fin=pd.DataFrame(y_ts_clf_fin, columns=['delayed'])\n",
    "    df_pred_fin['pred']= final_estimator.predict(X_ts_clf_fin)\n",
    "    # Make DataFrame of True Positives for regression prediction (length/extent of delay) \n",
    "    df_true_pos_fin = df_pred_fin[(df_pred_fin.delayed==1) & (df_pred_fin.delayed==df_pred_fin.pred)]\n",
    "    # Save to disk\n",
    "    path = os.curdir+\"\\Data\\Results\\\\\"\n",
    "    df_pred_fin.to_csv(path+'classifier_final_predicted.csv')\n",
    "    df_true_pos_fin.to_csv(path+'classifier_final_true_positives.csv') # Save to data folders\n",
    "    #-------------------Regression----------------#\n",
    "    reg_estimator_fin = regressor\n",
    "    # Get training and test sets...\n",
    "    X_tr_reg_fin, X_ts_reg_fin, y_tr_reg_fin, y_ts_reg_fin = train_test_conditional(data,\n",
    "                                                                delayed, df_true_pos_fin, ddate, delayed)\n",
    "    reg_estimator_fin.fit(X_tr_reg_fin,y_tr_reg_fin)\n",
    "    r2= r2_score(y_ts_reg_fin,reg_estimator_fin.predict(X_ts_reg_fin))\n",
    "    rmse = np.sqrt(mean_squared_error(y_ts_reg_fin,reg_estimator_fin.predict(X_ts_reg_fin)))\n",
    "    #d_rsq_fin = pd.DataFrame([r2,rmse], columns=['r2', 'rmse'])\n",
    "    return df_pred_fin, df_true_pos_fin,  d_feat_imp_clf_fin, clfreport, cmatrix, r2, rmse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277410a-6457-44c1-a685-a740d96ac896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
